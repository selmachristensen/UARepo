{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c1614d-2793-4237-a4d9-137d0f89a60b",
   "metadata": {},
   "source": [
    "# GG4257 - Urban Analytics: A Toolkit for Sustainable Urban Development\n",
    "## Lab Workbook No 2: Data Manipulation and Working with Web Services\n",
    "---\n",
    "Dr Fernando Benitez -  University of St Andrews - School of Geography and Sustainable Development - Iteration 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22552605-aaf7-400d-8b8f-256396bd43f5",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "- Practice and get familiar with the urban data landscape\n",
    "- Understand the *messiness* of real world (urban) data\n",
    "- Create common objects used to store data\n",
    "- Read tabular and spatial data formats into Python\n",
    "- Subset and merge data\n",
    "- Manipulate data and calculate new values\n",
    "- Practice basic techiques of Data Manipulation and learn how to use Python to access programmatically urban data and understand the benefits of it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6bad4-0c4d-4c6a-b085-e954304fb955",
   "metadata": {},
   "source": [
    "# Data Manipulation\n",
    "\n",
    "Access and manipulate data is very important when it comes to work with python or any kind of programming langague, you need to feel confortable by reading and manipulating data in this programatic way. Initially it will seems it is more complicated, but later with practice you will see the advangte of dealing with data in this way. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90602f00-7d62-4d60-bb3b-2b73721e4ccf",
   "metadata": {},
   "source": [
    "## Arrays and Data Frames (Numpy and Pandas)\n",
    "\n",
    "As you recall from the previous modules ( GG3209), there are two packages that can help you to read data in your computer memory and then let you manipulate them in a more efficient way. \n",
    "\n",
    "Two main object types that can be used to store tabular data in Python include the data frame and array. Each column of a data frame must be a single type, but different columns can be different types (e.g. string, float, etc.); all the columns of an array must be the same type. You can create these within Python manually or by reading in other common formats such as spreadsheets or csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f909796-1f27-4e38-9ca0-eae46066a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Create two arrays\n",
    "years = range(2010, 2018) #creates a list of consecutive integers from 2010 to 2017\n",
    "a = np.repeat(years, 4) #this uses numpy's repeat() function to repeat values\n",
    "b = np.random.randint(0, 40, 32) # the randint() function can be used to generate random integers - in this case 32 values between 0 and 40\n",
    "#Create data frame\n",
    "c = pd.DataFrame({'a':a,'b':b})  #the curly brackets indicate a dictionary\n",
    "c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929db3e1-8b0f-4220-b12d-7647ac00932d",
   "metadata": {},
   "source": [
    "The last line says to make a two column data frame, where the first column is called `'a'` and has the values from the `a` array, and the second column is called `'b'` and contains the values from the `b` array.\n",
    "\n",
    "You can type `c` into the console to return the whole data frame, however, you might just want to look at the top few rows. This can be achieved with the `head()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff000075-90e0-4c11-bb10-71c141112d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#head returns the top five rows\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dd944-2181-4ef3-b063-228583dc41be",
   "metadata": {},
   "source": [
    "In a similar way you can create arrays using the numpy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecfda8-f9e6-4d7d-9af5-ce7935d4c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of numbers\n",
    "a = range(0, 25) #the range function generates a range of integers\n",
    "b = np.array(a) #creates a one dimensional array\n",
    "b = b.reshape(5,5) #create an array with 5 rows and 5 columns\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ec082-9243-4c83-a5ee-bd555b8019ab",
   "metadata": {},
   "source": [
    "It is possible to multiply a numeric array by a constant or another array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dff3fe-d6c5-4198-be29-d925c0b8edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiply b by 10\n",
    "b * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592878d6-0e13-4d41-b240-23094e1c35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiply b * b\n",
    "b * b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19908527-f710-48fa-b51c-a50c4c576f20",
   "metadata": {},
   "source": [
    "Extracting elements from a one dimensional array is the same as a list; for a two dimensional array the slicing is formatted as [row number, column number]. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a7efa-8151-42f2-af84-fd9630c53657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the 0 position row\n",
    "b[0,:]  #the colon means give all elements along that dimension\n",
    "\n",
    "#Extract 3 position column\n",
    "b[:,3]\n",
    "\n",
    "#Extract the 2 and 3 position columns\n",
    "b[:,2:4] # The colon is used to define a numeric vector between the two numbers\n",
    "\n",
    "#Extract 0 and 3 position rows\n",
    "b[[0,3],:] # The list ([0,3]) is used to identify the indexes to be extracted\n",
    "\n",
    "#Extract the value in the 2 position row and 3 position column\n",
    "b[2,3]\n",
    "\n",
    "# If you run this cell you will get the output only from the last part of the script,\n",
    "# if you want to see the outcome of every part, just copy and paste the code in another code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef81c05-881e-4b18-b3d5-fda509033493",
   "metadata": {},
   "source": [
    "In the data frame that you created earlier, you can use a similar notation to extract values based on the row and column indexes. It is formatted as `.iloc[row index, column index]`.\n",
    "\n",
    "As you see in the previous practices, `iloc` and `loc` are key methods to explore, filter and manipulate data uisng pandas. So make sure you understand how that work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354da6c0-f017-467f-aff6-ea544938e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.iloc[23,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440e875-a74a-4bc7-a424-474b6580ec2d",
   "metadata": {},
   "source": [
    "Data frames can have named rows and columns, which can be used for indexing. It is formatted as `loc[row name, column name]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85b373-eb41-4400-a320-f80e4d5823f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.loc[23,'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65146c88-e292-4980-a5f9-e00cfa8a6752",
   "metadata": {},
   "source": [
    "You can also reference the column names themselves using dot notation, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf643b2-3c54-429d-bbec-a7ce1a34cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return all the values in the column called \"a\"\n",
    "c.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b15fd-9f87-426c-9f98-23f45f35fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A different way of returning the column called \"a\"\n",
    "c[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69a577-0180-4cc5-8206-b22ba3d2ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yet another way of returning the column called \"a\"\n",
    "c.loc[:,\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca980a65-3560-40e3-8ca7-949afc92505b",
   "metadata": {},
   "source": [
    "We can also find out what a data frame's column names are using the `columns` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d02a5c-480d-4ad1-8026-389491dd50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1dd84-7a14-459c-b18f-7a00f84cf020",
   "metadata": {},
   "source": [
    "There are many ways you can rename the columns, here one of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d13ac5-3d03-43fa-a748-c2d7fb831af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.rename(columns={'a': 'Year', 'b': 'Count'})\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64186c-78ab-40b3-b008-2e4558a531ad",
   "metadata": {},
   "source": [
    "# Challenge No 1:\n",
    "\n",
    "1. Using a Dictionary, create a dataframe (table), with at least 4 columns and more than 100 rows. How come you can create this among data from scratch without defining every single row of data? \n",
    "2. Using the appropriate method, create a new DataFrame containing only the first 30 rows and the first 3 columns of the original DataFrame. Name this new DataFrame subset_df.\n",
    "3. Using the appropriate method, filter the rows from the original dataframe where a numerical attribute(column) is greater than a particular numerical value, and find another categorical attribute that is equal to a specific string or text. Name this new DataFrame filtered_df.\n",
    "4. Check this website https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html and apply the methods, mean, standard deviation, group_by to run fundamental statistical analysis of your created data frame.\n",
    "5. Make sure you comment on your code and describe how you are manipulating the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfef5d-2020-4fbd-a8e5-7c82d221df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows you want\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rows = 120\n",
    "#based of of the datafram we were given before I created a dictionary using different numpy mathematical tools\n",
    "data = {\n",
    "    \"number\": range(rows), \n",
    "    \"random radints\": np.random.randint(0, 100, rows),  \n",
    "    \"decimals\": np.random.uniform(0, 1, rows),\n",
    "    \"lettering\": np.repeat([\"A\", \"B\", \"C\", \"D\"], rows // 4)\n",
    "}\n",
    "\n",
    "# Create DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f064b99-a996-413d-ab33-7f9b5879a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the appropriate method, create a new DataFrame containing only the first 30 rows and the first 3 columns of the original DataFrame. Name this new DataFrame subset_df.\n",
    "subset_df = df.iloc[:30, :3]\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aae2ba-01c9-4343-ac9b-1220a957a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the appropriate method, filter the rows from the original dataframe where a numerical attribute(column) is greater than a particular numerical value, and find another categorical attribute that is equal to a specific string or text. Name this new DataFrame filtered_df.\n",
    "numerical_attribute_condition = 50\n",
    "cattegorical_attribute = 'B'\n",
    "filtered_df = df[(df['random radints'] > numerical_attribute_condition) & (df['lettering'] == cattegorical_attribute)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf94807-6ee6-45dc-8ac4-0aabb2b96a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check this website https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html and apply the methods, mean, standard deviation, group_by to run fundamental statistical analysis of your created data frame.\n",
    "numbers_df = df.select_dtypes(include=[np.number])\n",
    "df_means = numbers_df.mean()\n",
    "df_standard_deviation = numbers_df.std()\n",
    "df_groupby = df.groupby('lettering').mean\n",
    "\n",
    "print(df_means)\n",
    "print(df_standard_deviation)\n",
    "print(df_groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e77da-1a7d-4b9d-8beb-4aa3ab9de8cc",
   "metadata": {},
   "source": [
    "## Reading External Data\n",
    "\n",
    "For most urban analytics you are more likely to be reading external data into Python rather than creating data objects from scratch. Tabular data is commonly stored in text files such as CSV, or on spreadsheets; and explicitly spatial data will likely be stored in formats such as Shapefiles.\n",
    "\n",
    "A common way in which data can be stored externally are the use of `.csv` files. These are text files, and have a very simple format where columns of attributes are separated by a comma, and each row by a carriage return.\n",
    "\n",
    "**Note:** There are a range of different delimiters which can be used in addition to a comma, with the most common being tab; although sometimes characters not commonly used such as bar/pipe (`|`) will be used.\n",
    "\n",
    "In the following example you will read in some U.S. Census Bureau, 2010-2014 American Community Survey (ACS) 5-Year Estimate data. This was downloaded from the [American Fact Finder](https://factfinder.census.gov) website. The data are for census tracts in San Francisco and relate to median earnings in the past 12 months.\n",
    "\n",
    "Reading CSV files into Python uses pandas `read_csv` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024788e-986a-413e-8c3d-47f241e9db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV file - creates a data frame called earnings\n",
    "earnings = pd.read_csv(\"ACS_14_5YR_S2001_with_ann.csv\")\n",
    "\n",
    "#Show column headings\n",
    "earnings.columns\n",
    "\n",
    "#UID - Tract ID\n",
    "#pop - estimated total population over 16 with income\n",
    "#pop_m - estimated total population over 16 with income (margin of error)\n",
    "#earnings - estimated median earnings\n",
    "#earnings_m - estimated median earnings (margin of error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e2b21-0c33-4ed0-a0b6-35052a0997da",
   "metadata": {},
   "source": [
    "It is possible to show the structure of the object using the `info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a449d0-6536-4249-8d88-54ca717bf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d1160-fdd9-4092-bae2-3df80e1bb4bf",
   "metadata": {},
   "source": [
    "This shows that the object is a Pandas data frame with 197 rows and 5 variables. For each of the attributes the class is shown (e.g. `int64` indicates integer). The `read_csv()` function guesses the column types when the data are read into Python.\n",
    "\n",
    "One issue you might notice is that the earnings and earnings_m variables have been read in as an `object`. The reason these columns were not read as integers (like the UID, pop, pop_m) is the presence of two non-numeric values which are shown as \"*\" and \"-\". In ACS data these two symbols indicate that the sample sizes were either no sample observations or too few sample observations to make a calculation.\n",
    "\n",
    "Issues such as these are quite common when reading in external data; and we will look at how this can be corrected later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95bae2-d425-414c-a9b6-12f6b5ea1e2e",
   "metadata": {},
   "source": [
    "**However!**: Not all tabular data are distributed as textfiles, and another very common format is Microsoft Excel format - .xls or xlsx.\n",
    "\n",
    "The following code downloads an Excel File from the [London Data Store](https://data.london.gov.uk/) and then reads this into Python.\n",
    "\n",
    ">Note: In the following code you will fetch data from a URL, something that is new at this point, I will describe better the importance of working with web services later in the next seccion of this workbook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7ae48-c58e-4684-ade5-0b053d633943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xlsx\"\n",
    "urllib.request.urlretrieve(url, \"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/UK_House_price_index.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a91cc7-b209-4de7-9a65-60a521e82bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read workbook\n",
    "house_price = pd.read_excel(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/UK_House_price_index.xlsx\", sheet_name='Average price')\n",
    "house_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b14ce-d232-49ba-98b2-5966515b8ac2",
   "metadata": {},
   "source": [
    "## Reading Spatial Data\n",
    "\n",
    "Spatial data are distributed in a variety of formats, but Shapefiles. are maybe the most common format. These can be read into Python using a number of packages, however, is illustrated here with \"geopandas\". You have experienced GeoPandas already in the previous module, but here you can practice once again.\n",
    "\n",
    "The following code loads the house composition from the latest Census in the UK downloaded from the [NOMIS Data](https://www.nomisweb.co.uk/datasets/c2021ts003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8150b-6f77-4f96-bd5c-b2fb4be1fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading house data \n",
    "import pandas as pd\n",
    "house_data = pd.read_excel(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/householdcomposition.xlsx\")\n",
    "pd.options.display.max_columns = None\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02c50d-0752-4a34-8165-63614a8de45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3d6b6-827d-4dfe-93d6-2fa72b3fd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9df79-16c3-45f6-8949-a11f5db00e5e",
   "metadata": {},
   "source": [
    "You have notice by using the `.info()` and .columns methods that our dataset have some issues than need to resolve before we movee fowards. The initial issue is the name of the columns, and the `Dytpe`\n",
    "of the columns, instead of having a string and numerical or float/intenger values, we have object, which tells us that there is something on those columns that might need to get removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fcd95-011e-4f6f-a507-2190030b06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by renaming some of the most important columns names.\n",
    "house_data.rename(columns={'2021 super output area - middle layer':'2021_MSOA_name','Unnamed: 1': 'MSOA21CD', 'Total: All households': 'All_households', 'One-person household':'1Person_household'}, inplace=True)\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28664968-7e43-4908-8954-f0b3c6d30130",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data['2021_MSOA_name'] = house_data['2021_MSOA_name'].astype(str)\n",
    "house_data['MSOA21CD'] = house_data['MSOA21CD'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05b649-f3da-44af-9be5-f8e25941520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e5d80-a5f8-4353-b877-4c2dbd02f04b",
   "metadata": {},
   "source": [
    "Here we are loading a shapefile from the latest version of census boundaries included in the The Open Geography portal from the Office for National Statistics (ONS) https://geoportal.statistics.gov.uk/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549b48d-75c2-4d42-ae78-381c314a5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading geopandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read Shapefile\n",
    "shapefile_path = \"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/MSOA_2021_BGC/MSOA_2021_EW_BGC_V2.shp\"\n",
    "gdf_shapefile = gpd.read_file(shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f6294-fba4-40a8-a576-478ce26e8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_shapefile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626bdf2-205a-4264-8ec7-a91b084f90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_shapefile.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ec60b-3e28-4d1c-a2cb-6975e7e1796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_shapefile.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e54b64-6399-4a1b-bbf1-3ea735a9741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_wgs84 = gdf_shapefile.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8be5b5-ac1f-4e72-ad34-b73bad903ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_wgs84.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7414c1-779e-4053-b039-67b43ab837ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_wgs84.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550667d-c166-437f-8484-4fcc9f4b9696",
   "metadata": {},
   "source": [
    "Now, we can also create a Dictionary to define which columns we would like to keep in a more curated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379e23b-6d4d-487b-af40-e4b1576a81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\n",
    "    \"MSOA21CD\",\n",
    "    \"MSOA21NM\",\n",
    "    \"geometry\",\n",
    "]\n",
    "msoa_shp = gdf_wgs84[keep_cols]\n",
    "msoa_shp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "msoa_shp.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9680f-c97f-408a-8dfd-fecb0c3d44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = msoa_shp.merge(house_data, on='MSOA21CD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58627e2e-0759-4a5c-b814-5425232961dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72eb620-ef1b-4aa4-a7fa-a73ff9289895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a \"magic\", which allows figures to be rendered inside the notebook (it only needs to be run once in Notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "merged_gdf.explore(column='All_households', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798458d-ed0e-4b26-9785-ff3131003ef9",
   "metadata": {},
   "source": [
    "As you already know, a geo data frame is structured like a regular data frame, with rows being observations and columns being attributes on those observations. The key difference is that a geo data frame has a `geometry` column that contains the spatial coordinates on each record. You can access the `geometry` like you would any column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884d4e0-2648-4d66-aab1-94cb9dab1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the top rows of the geometry column\n",
    "merged_gdf.geometry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the following line only the first time running this notebook.\n",
    "\n",
    "%pip install leafmap "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455ab53-c446-4ea6-b37e-d10bb2b7a24f",
   "metadata": {},
   "source": [
    "Leafmap is a popular new Python package for interactive mapping and geospatial analysis with minimal coding in a Jupyter environment.  Get more information in its documentation web site: https://leafmap.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e8487-9e49-46c6-9b11-7d16d27a394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap\n",
    "\n",
    "m = leafmap.Map()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4da1b-2122-4178-b69a-3d4447b9b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this handy web map to get the centre of the map, zoom level, and other great resources for web map development \n",
    "m = leafmap.Map(center=[54, -1], zoom=6)\n",
    "m.add_gdf(merged_gdf, layer_name=\"Housing\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e810d72-455d-4b5c-a9cb-7e66977e8e3e",
   "metadata": {},
   "source": [
    "## Creating Spatial Data\n",
    "\n",
    "Sometimes it is necessary to create a spatial object from scratch, which is most common for point data given that only a single co-ordinate is required for each feature. This can be achieved by building a `GeoDataFrame()` object and is used within this example to create a 311 point dataset. 311 data record non emergency calls within the US, and in this case are those which occurred within San Francisco between January and December 2016. The 311 data used here have been simplified from the [original](https://data.sfgov.org/City-Infrastructure/Case-Data-from-San-Francisco-311-SF311-/vw6y-z8j6/data) data to only a few variables, and those calls without spatial references have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75298641-4c33-43a1-8a2e-7f0d4fc5a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv into Python\n",
    "data_311 = pd.read_csv(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/311.csv\")\n",
    "# Have a look at the structure\n",
    "data_311.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22755f62-2fbe-4819-a997-26668e2c05c3",
   "metadata": {},
   "source": [
    "Each spatial object in a geo data frame must be a point, line or polygon. GeoPandas uses spatial object types from the shapely package. In this example we use the `Point` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d147a-6cff-471c-83cb-c0524fa78d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a geo data frame\n",
    "from shapely.geometry import Point  # import just the Point class from the shapely package\n",
    "geom = [Point(xy) for xy in zip(data_311.Lon, data_311.Lat)] #create a list of latitude, longitude pairs\n",
    "SP_311 = gpd.GeoDataFrame(data_311, crs=merged_gdf.crs, geometry=geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c700d-acd6-40d9-a9ef-ea03f063c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "SP_311.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb69983-ce42-47b1-a4c0-feb36cc12b2d",
   "metadata": {},
   "source": [
    "## Subsetting Data\n",
    "\n",
    "It is often necessary to subset data; either restricting a data frame to a set of columns or rows; or in the case of spatial data, creating an extract for a particular set of geographic features. Subsetting can occur in a number of different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c20128-fc8f-441b-9244-c58cb5e65e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the frequencies by the categories used within the 311 data\n",
    "data_311.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074253c-ec02-4b70-acb7-7a680734921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loc method to extract rows from the data which relate to Sewer Issues\n",
    "sewer_issues = data_311.loc[data_311.Category==\"Sewer Issues\", :]\n",
    "\n",
    "# Use the square brackets \"[]\" to perform the same task\n",
    "sewer_issues = data_311[data_311.Category==\"Sewer Issues\"]\n",
    "sewer_issues.head()  #check out the first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca4b2b-4608-472c-9863-08f0a7cd0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the IDs for the \"Sewer Issues\"\n",
    "sewer_issues_IDs = data_311.loc[data_311.Category==\"Sewer Issues\", \"CaseID\"]\n",
    "sewer_issues_IDs.head()  #check out the first elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934773d-9937-4613-be37-136b1424d4c9",
   "metadata": {},
   "source": [
    "Subsetting can also be useful for spatial data. In the example below we can list all the regions in England, and using simple queries we can remove, overwrite, and plot only certain rows inside our initial spatial data frame. For example consider you need to remove an area from your original dataset, and you you the code or value to use for the querie, so you can use something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b189c-0f02-436e-9e6f-3cb9abbecdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_england = gpd.read_file(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/Regions_(December_2021)/RGN_DEC_2021_EN_BGC.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8ee0e-c04a-440d-bbb6-be3434309e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_england.explore('RGN21NM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca14b05-6e6c-4f78-a002-d60efa6ef6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_name = regions_england['RGN21NM'].value_counts()\n",
    "regions_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09608f8b-086f-4668-8014-0dd11d1e55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_england[regions_england.RGN21NM != \"South East\"].plot() # Removes South East from the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7174dde-fd9a-409e-b8c5-4bebaa63f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_england[regions_england.RGN21NM == \"North East\"].plot() # Only plots North East"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b73a4-6e3c-4774-bf7e-3ec7c3253acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_england = regions_england[regions_england.RGN21NM != \"London\"] # Overwrites the regions_england object\n",
    "regions_england.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9b71e-ebaf-4985-a72a-f193fc043676",
   "metadata": {},
   "source": [
    "## Clipping Spatial Data\n",
    "\n",
    "Clipping is a process of subsetting using overlapping spatial data. The following code uses the outline of the coast of the U.S. to clip the boundaries of the Census Track Shapefile, another geodata frame object. Note: it is a little slow.\n",
    "\n",
    "The Census Tracts Shapefile was downloaded from the [SF OpenData](https://data.sfgov.org/Geographic-Locations-and-Boundaries/Census-2010-Tracts-for-San-Francisco/rarb-5ahf/data).\n",
    "\n",
    "> Note: You will find an error; you should be able to fix it at this level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26452b8a-0743-4e4b-b554-5da371f9da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in coastal outline (Source from - https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)\n",
    "coast = gpd.read_file(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/Census Tracts USA/cb_2015_us_county_500k.shp\")\n",
    "SF = gpd.read_file(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/Census Tracts USA/tl_2010_06075_tract10.shp\")\n",
    "\n",
    "coast_single = coast.unary_union  #merges the US counties into a single object\n",
    "SF_clipped_geoms = SF.intersection(coast_single) # Clip the SF spatial data frame object to the coastline - just returns geometries\n",
    "SF_clipped = SF.copy() #make a copy of the SF dataframe\n",
    "SF_clipped['geometry'] = SF_clipped_geoms #replace the old geometries with the clipped geometries\n",
    "SF_clipped = SF_clipped[SF_clipped.intersects(SF_clipped_geoms.unary_union)] #subset to just the observations in the clipped area\n",
    "\n",
    "#Plot the results\n",
    "SF_clipped.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbcdc2c-1611-485d-93d2-37639c6ea8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_single = SF_clipped.unary_union  #merges SF tracts into a single object\n",
    "SP_311_PIP = SP_311[SP_311.intersects(SF_single)] # Select the 331 points data that intersect with San Francisco\n",
    "SP_311_PIP.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac5167-1e53-48b7-8824-c3641c80c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_311_PIP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5418b3-dfc2-442c-9659-e1771c8694c5",
   "metadata": {},
   "source": [
    "## Merging Tabular Data\n",
    "\n",
    "So far you have utilized a single data frame or spatial object; however, it is often the case that in order to generate information, data from multiple sources are required. Where data share a common \"key\", these can be used to combine / link tables together. This might for example be an identifier for a zone; and is one of the reasons why most statistical agencies adopt a standard set of geographic codes to identify areas.\n",
    "\n",
    "In the earlier imported data \"earnings\" this included a UID column which relates to a Tract ID. We can now import an additional data table called bachelors - this also includes the same ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af25d53-69f2-4a21-af14-81a743e8ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV file - creates a data frame called earnings\n",
    "bachelors = pd.read_csv(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/ACS_14_5YR_S1501_with_ann.csv\")\n",
    "bachelors.head()\n",
    "\n",
    "#UID - Tract ID\n",
    "#Bachelor_Higher - Bachelor degree or higher %\n",
    "#Bachelor_Higher_m - Bachelor degree or higher % (margin of error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc3ef21-7188-4bc8-83cf-0735af35088b",
   "metadata": {},
   "source": [
    "Using the matching ID columns on both datasets we can link them together to create a new object with the `merge()` function in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f206d6-8690-45a5-94c0-8567ee211bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the merge\n",
    "SF_Tract_ACS = pd.merge(earnings, bachelors, left_on=\"UID\", right_on=\"UID\")\n",
    "SF_Tract_ACS = pd.merge(earnings, bachelors, on=\"UID\") # An alternative method to the above, but a shortened version as the ID columns are the same on both data frames\n",
    "#there are many more options - for more details type help(pd.merge)\n",
    "\n",
    "#The combined data frame now looks like\n",
    "SF_Tract_ACS.head() # shows the top of the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb0b18-8c3c-4b92-b8bc-a00b6090d383",
   "metadata": {},
   "source": [
    "## Removing and Creating Attributes\n",
    "\n",
    "It is sometimes necessary to remove variables from a tabular object or to create new values. In the following example we will remove some unwanted columns in the SF_clipped object, leaving just the zone id for each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28a1ed-7302-4f0b-892b-d0764e5faee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remind yourself what the data look like...\n",
    "SF_clipped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083201c-f43c-47f9-a2b7-f6a943fd6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_clipped = SF_clipped[[\"GEOID10\", \"geometry\"]] #Makes a new version of the geo data frame with just the values of the GEOID10 and geometry columns\n",
    "\n",
    "#The data frame within the data slot now looks as follows\n",
    "SF_clipped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660e729-e32c-41ca-886b-a6fab445f362",
   "metadata": {},
   "source": [
    "These tract ID are supposed to match with those in the \"SF_Tract_ACS\" object, however, if you are very observant you will notice that there is one issue; the above have a leading zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e55f9c-120e-4da5-9add-e39ef144da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_Tract_ACS.head() # show the top of the SF_Tract_ACS object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece9341-7e9d-4782-8377-9bc6059a62b3",
   "metadata": {},
   "source": [
    "As such, in this instance we will create a new column on the SF_Tract_ACS data frame with a new ID that will match the SF GEOID10 column. We can achieve this using the square brackets (`[]`) notation and will call this new variable \"GEOID10\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a7e76-d6f6-448b-9a49-15ebd106484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new variable with a leading zero\n",
    "SF_Tract_ACS['GEOID10'] = \"0\" + SF_Tract_ACS.UID.astype(str) #need to convert the UID column to strings before prepending the zero\n",
    "SF_Tract_ACS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c0424-7414-4875-a6d5-20b4a87851d5",
   "metadata": {},
   "source": [
    "If you remember from earlier in this practical, the earnings data had some values that were stored as \"objects\" rather than floats or integers, and the same is true for both the bachelors data; and now the combined `SF_Tract_ACS` object. We can check this again as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4fbca-e833-4644-81c4-d29a22751397",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_Tract_ACS.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2821d9-25c1-4946-a679-68099238911d",
   "metadata": {},
   "source": [
    "We can also remove the UID column. A quick way of doing this for a single variable is to use the `drop()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1289c4-6be5-4ceb-9e75-d0dab4930a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_Tract_ACS = SF_Tract_ACS.drop('UID', axis=1)  #axis=1 indicates to drop a column (axis=0 is for rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e89b38-7e2d-4785-86eb-fdc44712622c",
   "metadata": {},
   "source": [
    "We will now convert the object variables to numbers. The first stage will be to remove the \"-\" and \"**\" characters from the variables with the `replace` function, replacing these with NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b883a-3f19-47d6-aee7-960fa53cade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the \"-\" and \"*\" characters\n",
    "import numpy as np\n",
    "SF_Tract_ACS.loc[SF_Tract_ACS.earnings=='-', 'earnings'] = np.nan #replace the \"-\" values with NA\n",
    "SF_Tract_ACS.loc[SF_Tract_ACS.earnings_m=='**', 'earnings_m'] = np.nan #replace the \"**\" values with NA\n",
    "SF_Tract_ACS.loc[SF_Tract_ACS.Bachelor_Higher=='-', 'Bachelor_Higher'] = np.nan #replace the \"-\" values with NA\n",
    "SF_Tract_ACS.loc[SF_Tract_ACS.Bachelor_Higher_m=='**', 'Bachelor_Higher_m'] = np.nan #replace the \"-\" values with NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14941ed5-150b-45d3-9732-b7aab9bf18a6",
   "metadata": {},
   "source": [
    "We will now convert these to numeric values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e74523-4a38-4960-8227-640d2d41f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_Tract_ACS.earnings = SF_Tract_ACS.earnings.astype(float)\n",
    "SF_Tract_ACS.earnings_m = SF_Tract_ACS.earnings_m.astype(float)\n",
    "SF_Tract_ACS.Bachelor_Higher = SF_Tract_ACS.Bachelor_Higher.astype(float)\n",
    "SF_Tract_ACS.Bachelor_Higher_m = SF_Tract_ACS.Bachelor_Higher_m.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ec3e8-7480-4347-9ce6-ecbb5f175746",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_Tract_ACS.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555ec6a-c934-4826-a78f-2a9c36797e2e",
   "metadata": {},
   "source": [
    "Now all the variables other than the \"GEOID10\" are stored as integers or floats:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e84db-bb8e-4cbf-9d0c-681dd9155c72",
   "metadata": {},
   "source": [
    "## Merging Spatial Data\n",
    "\n",
    "It is also possible to join tabular data onto a spatial object (e.g. a geo data frame) in the same way as with regular data frames. In this example, we will join the newly created `SF_Tract_ACS` data onto the `SF_clipped` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8b945-2a03-418d-905f-8e90be5c7a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_clipped = pd.merge(SF_clipped, SF_Tract_ACS, on=\"GEOID10\") # merge\n",
    "SF_clipped.head() #show the attribute data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0328a-ec51-406e-a237-cfd72f683688",
   "metadata": {},
   "source": [
    "## Spatial Joins\n",
    "\n",
    "Earlier in this practical we created a geo data frame which we later cropped using the `intersects()` method to create the `SP_311_PIP` object. As a reminder of what this looks like it is plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e638199-ba26-4b14-a269-627ff6435659",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_311_PIP.explore(tiles=\"CartoDB positron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d362834-e53b-4614-9efa-f6fb04140c8c",
   "metadata": {},
   "source": [
    "We will now clean up the associated data frame by removing all of the attributes apart from the category and geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5e1d0-b116-4412-9329-591f202cb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_311_PIP = SP_311_PIP[[\"Category\", \"geometry\"]] #subset data\n",
    "SP_311_PIP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5af97-147e-46f0-8b00-ca344c00b208",
   "metadata": {},
   "source": [
    "The `intersects()` method was used to clip a dataset to an extent earlier, essentially a [point in polygon](https://en.wikipedia.org/wiki/Point_in_polygon) function. In contrast, the spatial join (`sjoin()`) function performs the point in polygon action and has a really useful feature that it also appends the attributes of the polygon to the point. For example, we might be interested in finding out which census tracts each of the 311 calls resides within. As such, we will implement another point in polygon analysis to create a new object `SF_clipped_311`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9508e9d-f458-4843-944e-0d74ad753126",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_clipped_311 = gpd.sjoin(SP_311_PIP, SF, how='inner') # point in polygon\n",
    "#Cleanup the attributes\n",
    "SF_clipped_311 = SF_clipped_311[[\"GEOID10\",\"Category\",\"geometry\"]]\n",
    "#Show the top rows of the data\n",
    "SF_clipped_311.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4b749-32ba-4e06-9fb1-d64bbe7885cb",
   "metadata": {},
   "source": [
    "## Writing and saving your processed data\n",
    "\n",
    "In order to share data it is often useful to write data frames or spatial objects back out of Python as external files. This is very simple, and Python supports multiple formats. In these examples, a CSV file and a Shapefile are both created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd09de-6884-46fd-bf2d-fa9368d0d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this example, we write out a CSV file with the geo data frame SF_clipped_311\n",
    "SF_clipped_311.to_csv(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/311_Tract_Coded.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded510f-f77c-4683-b93e-9054387e8c3a",
   "metadata": {},
   "source": [
    "This has created a CSV file \"311_Tract_Coded.csv\" in your working directory; we will use this in the next practical class - \"Basic SQL\".\n",
    "\n",
    "It is also possible to write out a Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a4d22-6672-401a-924a-ff3e823c98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will write out a Shapefile for San Francisco - note, as the column names are a little longer than are allowed within a Shapefile and as such are automatically shortened.\n",
    "SF_clipped.to_file(\"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/SF_clipped.shp\") #the default is Shapefile, but other spatial formats are supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28089b42-8c25-4c12-a167-3e42331784c6",
   "metadata": {},
   "source": [
    "# Working with Web Services\n",
    "\n",
    "Nowadays, you have access to spatial and numerical data from multiple data sources; many great spatial data sources will include another type of access that isn't often very popular among the spatial data scientist communities, as we tend to use downloaded data to make any analysis. We consider that only through downloaded data will we be able to work more efficiently. Somehow, those caveats are true. However, we also need to consider the rapid improvement in computational capabilities, the use of the Web and Cloud GIS services, and take advantage of the faster internet connections we can access nowadays. As you probably already know, spatial data come in multiple formats, whether the type of data we need to store or the natural phenomena we try to represent; numerous types of spatial web services serve various purposes. \n",
    "\n",
    "To get to that point, if we work with a window-interface tool like QGIS or ArcGIS, the connection to the spatial data is usually established on local directories; depending on the type of project, you might need to work using a direct connection to a particular spatial database. However, in this example, I will show you how to use web services, in particular spatial web services, to get some numerical and spatial data, load that into your computer memory, process the information and then get the outcomes you can later include as an outcome of any spatial analysis.\n",
    "\n",
    "Generally, you can think of a web service as a piece of software designed to allow the interaction and communication between different applications over the internet. That is the keystone! You know that the transmission of your data goes through the internet, which means that a web service enables the exchange of data and, in many cases, also functionality between multiple systems, allowing them to work together seamlessly. \n",
    "\n",
    "Web services use standard web protocols, such as HTTP, and often communicate using formats like XML or JSON. They enable interoperability, scalability, and integration in distributed computing environments.\n",
    "\n",
    "## Web Services in Urban Analytics:\n",
    "\n",
    "In urban analytics, web services facilitate the seamless exchange and analysis of geospatial data, which is vital for understanding and managing urban environments. Spatial Web services provide a platform for sharing geographic information, conducting spatial studies, and delivering real-time data to support urban planning, transportation management, environmental monitoring, and more decision-making processes.\n",
    "\n",
    "## Common Spatial Web Services:\n",
    "\n",
    "Having spatial web services standards is extremely important., the standards are the glue to geospatial information interoperability, and are used by thousands of organizations across the globe and represented in millions of lines of code. They are backed by international organizations, used in proposals, and implemented to speed up the process of innovation. If you want to explore all the geospatial standands, take a look at the OGC web https://www.ogc.org/standards/ IN the following section I will describe the most common spatial web services you migth find when you work with spatial data. \n",
    "\n",
    "### Web Map Services (WMS):\n",
    "\n",
    "A WMS services is a standard protocol for serving georeferenced **map images** over the web. It allows clients to request maps from a server, which then generates and returns the map images.\n",
    "\n",
    "### Web Feature Services (WFS):\n",
    "\n",
    "A WFS service is a standard for serving and exchanging **vector** data over the web. It enables clients to request and retrieve features (geospatial entities) from a server, supporting more complex data interactions compared to WMS, including the editing and symbology capabilites.\n",
    "\n",
    "### GeoJSON/JSON Web Services:\n",
    "\n",
    "These web services use the JSON format to transmit geospatial data. They are often employed for simple and lightweight data exchange, suitable for web applications and APIs. These web services are popular for transmitting spatial data between clients and servers due to their simplicity and human-readability.\n",
    "\n",
    "### RESTful APIs (Representational State Transfer):\n",
    "\n",
    "RESTful APIs follow the principles of REST architecture, providing a set of rules for building web services. They use standard **HTTP methods (GET, POST, PUT, DELETE)** for data manipulation and are often implemented in the form of web APIs. APIs are versatile and widely adopted in numerous geospatial applications and are widely implemented allowing developers to access, manipulate, and retrieve geographic data programmatically.\n",
    "\n",
    "### APIs\n",
    "\n",
    "Application Programming Interface (APIs) represent a 'gate' or otherwise a platform that enables a client (that is you) to interact with a server (for example [Glasgow Open Data](https://developer.glasgow.gov.uk/),  [opendata.bristol.gov.uk](https://opendata.bristol.gov.uk/)). \n",
    "\n",
    "According to @amazonWhatAPI:\n",
    "> In the context of APIs, the word Application refers to any software with a distinct function. Interface can be thought of as a contract of service between two applications. This contract defines how the two communicate with each other using requests and responses. Their API documentation contains information on how developers are to structure those requests and responses.\n",
    "\n",
    "The client's software (this might be R pr Python for example) sends a request to the server requesting specific data. The response is the data the client asked.\n",
    "\n",
    "More commonly, the client might be a mobile phone app (e.g. train network status app) and the server is the network operator's server.\n",
    "\n",
    "APIs can be private or public types. For more inthe description from @amazonWhatAPI [here](https://aws.amazon.com/what-is/api/#:~:text=API%20stands%20for%20Application%20Programming,other%20using%20requests%20and%20responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6b266-90c0-4c6a-970f-c9ae6850506e",
   "metadata": {},
   "source": [
    "In the following example, we will use the Glasglow Open Data API to fetch data from the bike rentals.\n",
    "1. Please go to https://developer.glasgow.gov.uk/\n",
    "2. Sign Up and explore the available APIs\n",
    "3. Go to https://developer.glasgow.gov.uk/api-details#api=mobility&operation=get-getrentals and explore the available parameters to fetch data from the Bike Rentals in Glasgow.\n",
    "4. To your right, you will see a tiny green button, **Try it**, where you can play with the API requests and see if you can get an appropriate response for the last 3 weeks of data. Help: Just add the parameter `3_weeks_ago` in the Value box and then click on the **Send** button to see how the API responds. This is what we will apply but using python to write some analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa75bd-0ab8-4d4b-bd38-d237eee8d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Let's describe the url, it is usually easier to do it like this, so in the future, you can easily update the URL\n",
    "url_bikes = \"https://api.glasgow.gov.uk/mobility/v1/get_rentals?startDate=2022-05-01&endDate=2023-05-01\"\n",
    "# Making the query to the web server, using the Get method from the requests library \n",
    "response = requests.get(url_bikes)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31651487-1d9b-4cd8-98e2-2ace76f75db7",
   "metadata": {},
   "source": [
    "You see the response has a 200 code, which means the request as satisfactory, here the possible other codes you can get and hence you can see if your code has any issue. https://www.w3schools.com/tags/ref_httpmessages.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3b57c-5720-4c59-8a2c-6244f95cf78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now we get the response from the web server, we need to translate that into a format we can manipulate, like JSON.\n",
    "data = response.json()\n",
    "data\n",
    "# careful here you will get a huge outcome; explore what you get, and then you can clear this cell outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130a87c-6a1c-427a-9eac-d98ae3bd7779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Usually, there are two labels into the web server response the metadata, and the data; we will use the data label\n",
    "# to get all attributes included. \n",
    "rental_data = data['data']\n",
    "rental_data\n",
    "# See the structure of the data, you can see\n",
    "# 'attribute':'value' structure\n",
    "# each {} define one row or one element\n",
    "# Again, here you will get a huge outcome; just explore what you get, and then you can clear this cell outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190da044-2a10-455b-b659-216bf2e3b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_pd = pd.DataFrame(rental_data)\n",
    "#Can you guess what we are doing here?\n",
    "rental_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ac452-9734-4d63-a926-ad4c5e5042d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5cb5c-4280-42cf-b37b-95d0c6bbdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489c83c-6de6-43f4-9683-2246d90244cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN in the coordinates column\n",
    "nan_in_column_Lat = rental_pd['startPlaceLat'].isna().any()\n",
    "nan_in_column_Long = rental_pd['startPlaceLong'].isna().any()\n",
    "\n",
    "print(nan_in_column_Lat,nan_in_column_Lat)\n",
    "\n",
    "# Alternatively, you can use the following to count NaN values\n",
    "nan_in_column_Lat = rental_pd['startPlaceLat'].isna().sum()\n",
    "nan_in_column_Long = rental_pd['startPlaceLong'].isna().sum()\n",
    "print(nan_in_column_Lat,nan_in_column_Lat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076b5e2-947a-42f2-b994-34ce1cce193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rental_pd = rental_pd.dropna(subset=['startPlaceLat', 'startPlaceLong', 'endPlaceLat','endPlaceLong'])\n",
    "clean_rental_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f1f00-1807-4328-80ae-50f085329998",
   "metadata": {},
   "source": [
    "Now, using the GeoPandas Documentation site, we can see how to build a Geodataframe using the Lat and Long attributes. This dataset includes two sets of coordinates, one for when the user gets the bike and another one for when the user returns the bike. \n",
    "\n",
    "https://geopandas.org/en/stable/gallery/create_geopandas_from_pandas.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11814a3b-4209-4fae-b04a-e2ccd45e261a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_bikes_start = gpd.GeoDataFrame(clean_rental_pd, geometry=gpd.points_from_xy(clean_rental_pd['startPlaceLong'], clean_rental_pd['startPlaceLat']))\n",
    "gdf_bikes_end = gpd.GeoDataFrame(clean_rental_pd, geometry=gpd.points_from_xy(clean_rental_pd['endPlaceLong'], clean_rental_pd['endPlaceLat']))\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "gdf_bikes_start.info()\n",
    "# Do we need all those columns? And you see, there is also a lot of pre-processing to do with all the object Dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42846329-294d-4951-a05d-dd66be9ea6bb",
   "metadata": {},
   "source": [
    "Let's plot one of the GeoPandasDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4adc35-1aed-45f5-b98e-3473537f1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd8235-ad92-4c38-a9ad-ebd54069e9ed",
   "metadata": {},
   "source": [
    "What is wrong with the previous map? why the points arent well located? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8579894-4095-4de5-84e0-2e2c715a595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf_bikes_start.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8e1b0-aeb0-400f-8853-005d553153aa",
   "metadata": {},
   "source": [
    "You see what the problem is?, let me fix that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db06e7-f57c-407f-b945-cdfbb97e1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start = gdf_bikes_start.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1360ae-1b39-44c1-a9e8-99b7b3092095",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a941e50-3584-4dd4-9e87-e66bf089fbd1",
   "metadata": {},
   "source": [
    "You could have fixed that problem from the moment you created the GeoPandasDataFrame, just follow the example included in the documentation link: https://geopandas.org/en/stable/gallery/create_geopandas_from_pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b88eb-9dfa-4f8f-8355-cf11cc387226",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ff24d-60b9-46d3-8f37-064fa694f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\n",
    "    \"startDate\",\n",
    "    \"startPlaceId\",\n",
    "    \"startPlaceName\",\n",
    "    \"durationSeconds\",\n",
    "    \"isInvalid\",\n",
    "    \"price\",\n",
    "    \"isEbike\",\n",
    "    \"startPlaceLat\",\n",
    "    \"startPlaceLong\",\n",
    "    \"geometry\",\n",
    "]\n",
    "gdf_bikes_start = gdf_bikes_start[keep_cols]\n",
    "gdf_bikes_start.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bcd0d-d222-4bbb-9a79-101768c3fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bd32a-cee6-482d-9dde-72ce0c2a2108",
   "metadata": {},
   "source": [
    "Updating the requiered and more appropiated Dtypes for the remainng columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d08c9b-a61b-4097-aefe-f15bf7490f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start.startPlaceId = gdf_bikes_start.startPlaceId.astype(int)\n",
    "gdf_bikes_start.startPlaceName = gdf_bikes_start.startPlaceName.astype(str)\n",
    "gdf_bikes_start['startDate'] = pd.to_datetime(gdf_bikes_start['startDate'], format='%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b933fc3-c78f-4155-8e1b-91d7f9540518",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start.dtypes\n",
    "#gdf_bikes_start['startPlaceName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3a11f-e247-4349-953b-527896a82d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73919b85-1303-46e5-aa77-76bbbcd28525",
   "metadata": {},
   "source": [
    "Now, we want to see where the more dense areas are and where the bikes get collected so that we will use a simple but straightforward cluster analysis. We will explore this in more detail later in this course; for now, let's apply an ML library in Python sklearn (https://scikit-learn.org/stable/index.html) and define only 4 cluster areas. We will use the geometry attribute to get our Lat and Long values, which are required for the sklearn library fit_predict method.\n",
    "\n",
    "Before that, let's explore how we get the Lat and the Long values in the way the cluster method requires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121afd45-1382-4a0e-a2bf-11c8e5dfdac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 4\n",
    "\n",
    "kmeans_collection = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "gdf_bikes_start['kmeans_cluster'] = kmeans_collection.fit_predict(gdf_bikes_start[['startPlaceLong', 'startPlaceLat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebddb74d-ea1f-4a06-b246-27fb2bc4fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_start.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc6442-0130-4149-96e3-170dac69dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap\n",
    "\n",
    "m = leafmap.Map(center=(55.860166, -4.257505),\n",
    "                zoom=12,\n",
    "                draw_control=False,\n",
    "                measure_control=False,\n",
    "                fullscreen_control=False,\n",
    "                attribution_control=True,\n",
    "                   \n",
    "               )\n",
    "\n",
    "m.add_basemap(\"CartoDB.Positron\")\n",
    "m.add_data(\n",
    "    gdf_bikes_start,\n",
    "    column='kmeans_cluster',\n",
    "    legend_title='Clusters',\n",
    "    cmap='Set1',\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "#Ploting the map\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1616b11-f7ed-4608-8fd6-4f0c23ceeddf",
   "metadata": {},
   "source": [
    "# Challenge No 2:\n",
    "\n",
    "**Part No 1:**\n",
    "\n",
    "1. Using the same workflow previously described, now calculate the clustered areas for the GeoPandasDataFrame `gdf_bikes_end`\n",
    "2. Make sure you don't have any NaN in your columns, add a CRS, clean up the unnecessary attributes, calculate the cluster values, and plot a map of 4 calculated clusters for the return locations.\n",
    "\n",
    "**Part No 2:**\n",
    "\n",
    "1. Using the Glasglow Open Data API ( Transit) https://developer.glasgow.gov.uk/api-details#api=traffic&operation=traffic-sensor-locations fetch all the sensor locations in the city.\n",
    "2. Map the sensor\n",
    "3. Find the WorkingZones and Calculate/Map the areas with more and fewer sensors distributed in the city.\n",
    "4. You will need:\n",
    "   * Get two separate Geopandas DataFrames, one for the traffic sensors and another one for the WorkingZones.\n",
    "   * Using `sJoin` ( Spatial Join) https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html\n",
    "   calculate the overlay of sensors and polygons.\n",
    "   * Using group_by https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html to count the number of sensors per WorkingZone\n",
    "   * Make sure you add the counts into the WorkingZone polygons of Glasgow so you can create a map of Zones with more and fewer traffic sensors.\n",
    "   * Of course, you will need extra steps where you manipulate the data and extract what you need, for instance, clipping the Working Zones only for Glasgow.\n",
    "5. Make sure you comment on your code and describe how you are manipulating the data.\n",
    "\n",
    "\n",
    "## Reading a WMS Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab29fb0-991a-43db-aa9b-e0d3207a7cb0",
   "metadata": {},
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b8fd9-ce22-4720-b572-66b4134f6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# Using the same workflow previously described, now calculate the clustered areas for the GeoPandasDataFrame `gdf_bikes_end`\n",
    "# Make sure you don't have any NaN in your columns, add a CRS, clean up the unnecessary attributes, calculate the cluster values, and plot a map of 4 calculated clusters for the return locations.\n",
    "\n",
    "# assign the correct crs\n",
    "\n",
    "gdf_bikes_end = gpd.GeoDataFrame(clean_rental_pd, geometry=gpd.points_from_xy(clean_rental_pd['endPlaceLong'], clean_rental_pd['endPlaceLat']))\n",
    "gdf_bikes_end = gdf_bikes_end.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf2e7d-6528-4549-9720-384964582ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN in the coordinates column\n",
    "nan_in_column_Lat = rental_pd['endPlaceLat'].isna().any()\n",
    "nan_in_column_Long = rental_pd['endPlaceLong'].isna().any()\n",
    "\n",
    "print(nan_in_column_Lat,nan_in_column_Lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78916879-ddba-4946-ae5a-138730484132",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rental_pd = rental_pd.dropna(subset=['endPlaceLat','endPlaceLong'])\n",
    "clean_rental_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84499ca-dbc4-43c9-b62b-139992d6956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_end.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be111e37-cec7-4948-8907-053c7768961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based off of what columns were kept befor I filtered the columns\n",
    "keep_cols = [\n",
    "    \"endDate\",\n",
    "    \"endPlaceId\",\n",
    "    \"endPlaceName\",\n",
    "    \"durationSeconds\",\n",
    "    \"isInvalid\",\n",
    "    \"price\",\n",
    "    \"isEbike\",\n",
    "    \"endPlaceLat\",\n",
    "    \"endPlaceLong\",\n",
    "    \"geometry\",\n",
    "]\n",
    "gdf_bikes_end = gdf_bikes_end[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aef02d-a1aa-41ee-972a-0d9bc16e7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking out what the data set looks like\n",
    "gdf_bikes_end.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399f9a3-f16f-431c-8ba1-51ada8476872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure if they are the right data types\n",
    "gdf_bikes_end.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee17bf6-d653-4383-8e5a-1d88ea0f3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the data to become the right types\n",
    "gdf_bikes_end.endPlaceId = gdf_bikes_end.endPlaceId.astype(int)\n",
    "gdf_bikes_end.endPlaceName = gdf_bikes_end.endPlaceName.astype(str)\n",
    "gdf_bikes_end['endDate'] = pd.to_datetime(gdf_bikes_end['endDate'], format='%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20830eb6-729a-47c9-a185-e01ffad19b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_bikes_end.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23d503-3947-43e2-baaf-2918a918d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluste calculations\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 4\n",
    "\n",
    "kmeans_collection = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "gdf_bikes_end['kmeans_cluster'] = kmeans_collection.fit_predict(gdf_bikes_end[['endPlaceLong', 'endPlaceLat']])\n",
    "gdf_bikes_end.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26893518-1693-4456-9e58-e4606b405ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look into parameters - dont think they are necessarily the same for both -- no clusters are showing up\n",
    "import leafmap\n",
    "\n",
    "endmap = leafmap.Map(center=(55.860166, -4.257505),\n",
    "                zoom=12,\n",
    "                draw_control=False,\n",
    "                measure_control=False,\n",
    "                fullscreen_control=False,\n",
    "                attribution_control=True,\n",
    "                   \n",
    "               )\n",
    "\n",
    "endmap.add_basemap(\"CartoDB.Positron\")\n",
    "endmap.add_data(\n",
    "    gdf_bikes_end,\n",
    "    column='kmeans_cluster',\n",
    "    legend_title='Clusters',\n",
    "    cmap='Set1',\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "#Ploting the map\n",
    "endmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea749cb-10dc-4099-9d33-a5842aa2b26f",
   "metadata": {},
   "source": [
    "\n",
    "1. Using the Glasglow Open Data API ( Transit) https://developer.glasgow.gov.uk/api-details#api=traffic&operation=traffic-sensor-locations fetch all the sensor locations in the city.\n",
    "2. Map the sensor\n",
    "3. Find the WorkingZones and Calculate/Map the areas with more and fewer sensors distributed in the city.\n",
    "4. You will need:\n",
    "   * Get two separate Geopandas DataFrames, one for the traffic sensors and another one for the WorkingZones.\n",
    "   * Using `sJoin` ( Spatial Join) https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html\n",
    "   calculate the overlay of sensors and polygons.\n",
    "   * Using group_by https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html to count the number of sensors per WorkingZone\n",
    "   * Make sure you add the counts into the WorkingZone polygons of Glasgow so you can create a map of Zones with more and fewer traffic sensors.\n",
    "   * Of course, you will need extra steps where you manipulate the data and extract what you need, for instance, clipping the Working Zones only for Glasgow.\n",
    "5. Make sure you comment on your code and describe how you are manipulating the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eca3e-534f-4094-a053-8e239abfd2a8",
   "metadata": {},
   "source": [
    "# Challenge 2: Part 2\n",
    "Map the sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d924c-9813-4ea5-9b74-07649d90b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "# Using the Glasglow Open Data API ( Transit) \n",
    "# https://developer.glasgow.gov.uk/api-details#api=traffic&operation=traffic-sensor-locations fetch all \n",
    "# the sensor locations in the city.\n",
    "\n",
    "url_sensors = \"https://api.glasgow.gov.uk/traffic/v1/movement/sites\"\n",
    "# Making the query to the web server, using the Get method from the requests library \n",
    "response = requests.get(url_sensors)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752ef38-4e27-4a73-b215-2d1fc2e8bd13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#translate data into a readable format\n",
    "data = response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdec12-53e8-45ba-b65f-007d46d9e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as data appeared as lists within eachother it needed to be converted into a readable dataframe\n",
    "#first I created empty lists for each variable within the data set\n",
    "siteIds = []\n",
    "from_descriptions = []\n",
    "from_lats = []\n",
    "from_longs = []\n",
    "to_descriptions = []\n",
    "to_lats = []\n",
    "to_longs = []\n",
    "\n",
    "#then I created a for loop which for every entry within the variable would append the value \n",
    "for entries in data:\n",
    "    siteId = entries['siteId']\n",
    "    from_description = entries['from']['description']\n",
    "    from_lat = entries['from']['lat']\n",
    "    from_long = entries['from']['long']\n",
    "    to_description = entries['to']['description']\n",
    "    to_lat = entries['to']['lat']\n",
    "    to_long = entries['to']['long']\n",
    "\n",
    "    siteIds.append(siteId)\n",
    "    from_descriptions.append(from_description)\n",
    "    from_lats.append(from_lat)\n",
    "    from_longs.append(from_long)\n",
    "    to_descriptions.append(to_description)\n",
    "    to_lats.append(to_lat)\n",
    "    to_longs.append(to_long)\n",
    "\n",
    "# Finally I created a Pandas data frame, assigning each column with a value coresponding with the previous list\n",
    "    sensor_data = pd.DataFrame({\n",
    "    'siteId': siteIds,\n",
    "    'from_description': from_descriptions,\n",
    "    'from_latitude': from_lats,\n",
    "    'from_longitude': from_longs,\n",
    "    'to_description': to_descriptions,\n",
    "    'to_latitude': to_lats,\n",
    "    'to_longitude': to_longs\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "sensor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f6439-3337-41d0-96c7-f8a854df45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I then checked to make sure that each column had the appropriate data type classification\n",
    "sensor_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39343512-b89b-4cf3-b371-0d07f8e84f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correscting the ones which didn't I changed the data to become the right type\n",
    "sensor_data.siteId = sensor_data.siteId.astype(str)\n",
    "sensor_data.from_description = sensor_data.from_description.astype(str)\n",
    "sensor_data.from_latitude = sensor_data.from_latitude.astype(float)\n",
    "sensor_data.from_longitude = sensor_data.from_longitude.astype(float)\n",
    "sensor_data.to_description = sensor_data.to_description.astype(str)\n",
    "sensor_data.to_latitude = sensor_data.to_latitude.astype(float)\n",
    "sensor_data.to_longitude = sensor_data.to_longitude.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71115c-7297-4717-a403-8d86868a0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next I checked to make sure this was the case\n",
    "sensor_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7dacf-9d5f-49de-8bdf-23379bbdb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828cb22-6c89-4668-b08a-8049dba02511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I then created two seperate geo data frames - although only one was needed I wanted to have a look at both \n",
    "from_sensor_data = gpd.GeoDataFrame(sensor_data, geometry=gpd.points_from_xy(sensor_data['from_longitude'], sensor_data['from_latitude']))\n",
    "to_sensor_data = gpd.GeoDataFrame(sensor_data, geometry=gpd.points_from_xy(sensor_data['to_longitude'], sensor_data['to_latitude']))\n",
    "\n",
    "# Print the GeoDataFrame\n",
    "from_sensor_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff970d2-c6b5-4e37-a80b-9f505f89c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clearing up the collumns I ccreated to seperate sensor datas, one for the from values and one for the to\n",
    "keep_cols_from = [\n",
    "    \"siteId\",\n",
    "    \"from_description\",\n",
    "    \"from_latitude\",\n",
    "    \"from_longitude\",\n",
    "    \"geometry\",\n",
    "]\n",
    "from_sensor_data = from_sensor_data[keep_cols_from]\n",
    "\n",
    "keep_cols_to = [\n",
    "    \"siteId\",\n",
    "    \"to_description\",\n",
    "    \"to_latitude\",\n",
    "    \"to_longitude\",\n",
    "    \"geometry\",\n",
    "]\n",
    "to_sensor_data = to_sensor_data[keep_cols_to]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f2a57-f7e3-43ca-b63c-6ad44c733279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assigned both with the correct glasweegan CRS\n",
    "from_sensor_data = from_sensor_data.set_crs(\"EPSG:4326\")\n",
    "to_sensor_data = to_sensor_data.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e93800-d2e5-4f2a-af5c-f08bc865db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next I mapped the sensor data, realizing that they both needed to be clipped to only include data from Glasgow\n",
    "from_sensor_data.explore()\n",
    "to_sensor_data.explore()\n",
    "\n",
    "#to do so I imported the working zone file as it only included scotland, thereby cutting out the data points elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7b973-303e-4463-b762-90f2ad410cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "# Read Shapefile (* Get two separate Geopandas DataFrames, one for the traffic sensors and another one for the WorkingZones)\n",
    "workingzone = \"/Users/selmachristensen/Documents/UA Repo1/Lab 2 data/WorkplaceZones2011Scotland/WorkplaceZones2011Scotland.shp\"\n",
    "gdf_workingzone = gpd.read_file(workingzone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40026cda-efd4-46a6-a3a2-01c1672ed4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_workingzone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e4cdb-8401-43cd-abd1-98530373996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_workingzone.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220e800-8840-49ab-9cfe-0aff8efb7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to join together both dataframes need to have the same crs\n",
    "print(from_sensor_data.crs)\n",
    "print(gdf_workingzone.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb5d23-da46-43ab-bb90-d75b72a0afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_workingzone = gdf_workingzone.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3005108-9066-4b06-8443-99035f4824f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using `sJoin` ( Spatial Join) https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html calculate the overlay of sensors and polygons.\n",
    "joint_workingzone_sensor = gpd.sjoin(from_sensor_data, gdf_workingzone, how='inner', op='intersects')\n",
    "joint_workingzone_sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16441ffb-7245-42eb-a932-6de56a1d5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joint_workingzone_sensor.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe810c-5398-4293-95fd-d508b6b900cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at this again\n",
    "#using the groupby function to calculate the number of sensors per WorkingZone\n",
    "\n",
    "groupby_info = joint_workingzone_sensor.groupby(by='WZCD')\n",
    "sensor_counts = groupby_info.size().reset_index(name='SensorCount')\n",
    "sensor_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9a88a-07e9-4cd1-915e-ca940af3ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "workingzone_with_counts = gdf_workingzone.merge(sensor_counts, how='left', left_on=['WZCD'], right_on=['WZCD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692df245-ba32-472a-96ce-5e21b7784759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#look into parameters - dont think they are necessarily the same for both -- no clusters are showing up\n",
    "import leafmap\n",
    "\n",
    "# Ensure 'WZCD' is unique in workingzone_with_counts\n",
    "workingzone_with_counts = workingzone_with_counts.groupby('WZCD').agg({'geometry': 'first', 'SensorCount': 'sum'}).reset_index()\n",
    "\n",
    "# Set the active geometry column\n",
    "workingzone_with_counts = workingzone_with_counts.set_geometry('geometry')\n",
    "\n",
    "# Create Leafmap\n",
    "workingzone_sensor_map = leafmap.Map(center=(55.860166, -4.257505), zoom=12, draw_control=False)\n",
    "\n",
    "# Add basemap\n",
    "workingzone_sensor_map.add_basemap(\"CartoDB.Positron\")\n",
    "\n",
    "# Add data\n",
    "workingzone_sensor_map.add_data(\n",
    "    workingzone_with_counts,\n",
    "    column='SensorCount',\n",
    "    legend_title='Amount',\n",
    "    cmap='viridis',  # Change colormap as needed\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "# Display the map\n",
    "workingzone_sensor_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502314d-c5fc-4f82-a3fb-3d7a201ce65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above map needs another look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea852f3-4b1f-45d1-ae82-c03b444775dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map(\n",
    "    center=(56.329031,-3.798943),\n",
    "    zoom=7\n",
    ")\n",
    "wms_url = 'https://maps.gov.scot/server/services/NRS/Census2011/MapServer/WMSServer?'\n",
    "# A WMS URL include multiple layers, so you need to provide the name you need to load in your map.\n",
    "# See this: https://www.spatialdata.gov.scot/geonetwork/srv/eng/catalog.search#/metadata/ff882746-e913-4f78-862e-f6e3974fb80e\n",
    "\n",
    "\n",
    "m.add_wms_layer(url=wms_url, layers='WorkplaceZones2011', name='Census2011', shown=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f71520-673d-472a-9260-001b71fd90d4",
   "metadata": {},
   "source": [
    "# Finishing the Lab\n",
    "\n",
    "Make sure you save all your code and upload the latest version of this notebook in your GitHub Repo. If you havent created a Repo to store all your Jupyter Notebooks related to the Labs, make sure you create a well and organized GitHub repo where you have the most curated and finished notebooks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
